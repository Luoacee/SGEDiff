import lmdbimport numpy as npfrom torch.utils.data import Datasetfrom torch_geometric.data import Datafrom torch_geometric.loader import DataLoaderimport osimport picklefrom utils.transforms import FeaturizeLigandAtom, FeaturizeProteinAtom, FeaturizeLigandBond, NormalizeVina, \    InteractionEx, FeaturizeProteinBondimport torchimport tqdmimport pickleclass BasedData(Data):    def __init__(self, *args, **kwargs):        super().__init__(*args, **kwargs)class DataSubset:    def __init__(self, lmdb_dir, save_path="data/SubSet.data", exclude_keys=None,                 hidden_progress=True, save_no_interaction=False):        self.data_dir = lmdb_dir        self.save_no_interaction = save_no_interaction        self.db = None        self.data = None        if exclude_keys is None:            self.exclude_keys = [""]        else:            self.exclude_keys = exclude_keys        self.progress = hidden_progress        self.save_path = save_path        self.open_lmdb()        self.process()        # self.split_dataset = self.train_test_split()        self._auto_save()    def open_lmdb(self):        self.db = lmdb.open(self.data_dir,                            map_size=20 * (1024 * 1024 * 1024),                            create=False,                            subdir=False,                            readonly=False,                            lock=True,                            readahead=False,                            meminit=False                            )    def process(self):        with self.db.begin(write=False) as txn:            cursor = txn.cursor()            # data_keys = [i.decode() for i in cursor.iternext(values=False)][:20]            data_keys = [i.decode() for i in cursor.iternext(values=False)]            # data_keys = data_keys[:len(data_keys) // 2]            # print("Data Reading: {%s}" % len(data_keys))            prop_names = pickle.loads(txn.get(data_keys[0].encode())).keys()            data_pocket = []            for key in tqdm.tqdm(data_keys, total=len(data_keys), disable=self.progress):                data = {}  # 补充data数据点                read_item = pickle.loads(txn.get(key.encode()))                # data_point = BasedData()                for itm in prop_names:                    if itm == "name":                        # self.__getattribute__(itm).append(key)                        # setattr(data_point, itm, key)                        data[itm] = key                    elif itm in self.exclude_keys:                        continue                    elif itm in ["interaction", "datasets", "ligand_atom_hybridization", "smiles", "pn", 'ln']:                        # setattr(data_point, itm, read_item[itm])                        data[itm] = read_item[itm]                    elif itm in ["qed", "logp", "tpsa"]:                        rd = np.around(read_item[itm], decimals=4)                        data[itm] = rd                    elif itm == "ligand_center":                        data[itm] = torch.from_numpy(np.array([read_item["ligand_center"]]))  # BATCH                    elif itm == "smiles_repr":                        data[itm] = torch.from_numpy(np.array(read_item["smiles_repr"]))                    elif itm == "sar3":                        data["adt"] = read_item["sar3"]["ADT"]                        data["dili"] = read_item["sar3"]["DILI"]                        data["restox"] = read_item["sar3"]["Respiratory_toxicity"]                        data["herg"] = read_item["sar3"]["hERG_1uM"]                        data["nep"] = read_item["sar3"]["Nephrotoxicity"]                    else:                        # self.__getattribute__(itm).append(re ad_item[itm])                        rd = torch.from_numpy(np.array(read_item[itm]))                        # setattr(data_point, itm, torch.from_numpy(rd))                        data[itm] = rd                data_combine = BasedData(**data)                if self.save_no_interaction:                    try:                        data_combine.interaction                    except Exception as e:                        setattr(data_combine, "interaction", [])                    data_pocket.append(data_combine)                else:                    try:                        x = data_combine.interaction                        data_pocket.append(data_combine)                    except Exception as e:                        pass        self.db.close()        self.data = data_pocket    def __call__(self, *args, **kwargs):        return self.data    # def __repr__(self):    #     return "Read dataset: %s, train: %s, test %s" % (len(self.data), self.train_num, self.test_num)    def train_test_split(self):        train = []        test = []        assert self.data is not None, "Data not exist!"        for dt in self.data:            if dt.datasets == "train":                train.append(dt)            else:                test.append(dt)        setattr(self, "train_num", len(train))        setattr(self, "test_num", len(test))        return train, test    def prop_init(self, prop_names):        for n in prop_names:            self.__setattr__(n, list())        return prop_names    def _auto_save(self):        with open(self.save_path, "wb") as f:            pickle.dump(self.data, f)    # def _auto_save(self):    #     with open("data/SubSet.data", "wb") as f:    #         pickle.dump({"train":self.split_dataset[0], "test":self.split_dataset[-1]}, f)class DataCollate(Dataset):    def __init__(self, dataset, transform=None):        super().__init__()        self.dataset = dataset        self.transform = transform    def __len__(self):        return len(self.dataset)    def __getitem__(self, index):        if self.transform is not None:            return self.transform(self.dataset[index])        else:            return self.dataset[index]